{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwXxKT7uvy5V"
   },
   "source": [
    "### Assignment 2 ###\n",
    "### Name - Pranav Gupta ###\n",
    "### Roll No. - 2021101095 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r6Oxb97Vvy5W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/rkada/miniconda3/envs/deepfont/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fJHDXeAvy5X"
   },
   "source": [
    "#### 1. SIFT-BoVW-SVM ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRZWpWVuvy5X"
   },
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "torch.manual_seed = 42\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouP8NvHEvy5X"
   },
   "source": [
    "##### 1.1 Implementation of BoVW Approach #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47uKWpQTvy5X"
   },
   "outputs": [],
   "source": [
    "def extract_sift_features(image):\n",
    "    image = np.uint8(image)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def cluster_sift_descriptors(descriptors, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(descriptors)\n",
    "    return kmeans, kmeans.cluster_centers_\n",
    "\n",
    "def image_histogram_represent(image, kmeans, num_clusters, cluster_centers):\n",
    "    image = np.uint8(image)\n",
    "    keypoints, descriptors = extract_sift_features(image)\n",
    "    histogram = np.zeros(num_clusters)\n",
    "    if descriptors is not None:\n",
    "        for descriptor in descriptors:\n",
    "            distances = np.linalg.norm(cluster_centers - descriptor, axis=1)\n",
    "            closest_visual_word_index = np.argmin(distances)\n",
    "            histogram[closest_visual_word_index] += 1\n",
    "    return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUEBaGf9vy5X"
   },
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for image, label in train_dataset:\n",
    "    y_train.append(label)\n",
    "\n",
    "y_test = []\n",
    "for image, label in test_dataset:\n",
    "    y_test.append(label)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwmPi3-nvy5Y"
   },
   "outputs": [],
   "source": [
    "# Feature Extraction from Training images\n",
    "descriptors = []\n",
    "for i in range(train_dataset.data.shape[0]):\n",
    "    keypoints, desc = extract_sift_features(train_dataset.data[i])\n",
    "    if desc is not None:\n",
    "        for i in range(len(desc)):\n",
    "            descriptors.append(desc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLOO86rAvy5Y"
   },
   "outputs": [],
   "source": [
    "# Fit K-Means on the obtained Descriptors\n",
    "k = 50\n",
    "kmeans, cluster_centers = cluster_sift_descriptors(np.array(descriptors), k)\n",
    "\n",
    "# Represent Train images as histograms\n",
    "svm_train = []\n",
    "for i in range(train_dataset.data.shape[0]):\n",
    "    histogram = image_histogram_represent(train_dataset.data[i], kmeans, k, cluster_centers=cluster_centers)\n",
    "    svm_train.append(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC2sAIAYvy5Y",
    "outputId": "59df6b95-5338-4515-c506-66e805e525b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;linearsvc&#x27;, LinearSVC(dual=&#x27;auto&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;linearsvc&#x27;, LinearSVC(dual=&#x27;auto&#x27;))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LinearSVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.LinearSVC.html\">?<span>Documentation for LinearSVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC(dual=&#x27;auto&#x27;)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('linearsvc', LinearSVC(dual='auto'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm1 = make_pipeline(StandardScaler(), svm.LinearSVC(dual = 'auto'))\n",
    "svm1.fit(svm_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noG4qz4lvy5Y",
    "outputId": "c2010e45-2b07-496a-c9cb-2566e851a423"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6475"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Represent Test Images as histograms\n",
    "svm_test = []\n",
    "for i in range(test_dataset.data.shape[0]):\n",
    "    histogram = image_histogram_represent(test_dataset.data[i], kmeans, k, cluster_centers=cluster_centers)\n",
    "    svm_test.append(histogram)\n",
    "\n",
    "# Find out the Classification Accuracy of the Model\n",
    "y_pred = svm1.predict(svm_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxIJg62Evy5Y"
   },
   "source": [
    "##### 1.2 Accuracy as a function of Number of Clusters #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5buTz8dvy5Y",
    "outputId": "f8419137-8156-4cef-aad1-3af4b50f7745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Clusters = 1\t-->\tAccuracy = 0.1926\n",
      "Number of Clusters = 2\t-->\tAccuracy = 0.217\n",
      "Number of Clusters = 5\t-->\tAccuracy = 0.3224\n",
      "Number of Clusters = 10\t-->\tAccuracy = 0.4004\n",
      "Number of Clusters = 20\t-->\tAccuracy = 0.5015\n",
      "Number of Clusters = 50\t-->\tAccuracy = 0.6651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1926, 0.217, 0.3224, 0.4004, 0.5015, 0.6651]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_list = [1, 2, 5, 10, 20, 50]\n",
    "accuracies = []\n",
    "for k in k_list:\n",
    "    print(f\"Number of Clusters = {k}\", end = '\\t')\n",
    "\n",
    "    # Fit K-Means on the obtained Descriptors\n",
    "    kmeans, cluster_centers = cluster_sift_descriptors(np.array(descriptors), k)\n",
    "\n",
    "    # Represent Train images as histograms\n",
    "    svm_train = []\n",
    "    for i in range(train_dataset.data.shape[0]):\n",
    "        histogram = image_histogram_represent(train_dataset.data[i], kmeans, k, cluster_centers=cluster_centers)\n",
    "        svm_train.append(histogram)\n",
    "\n",
    "    svm1 = make_pipeline(StandardScaler(), svm.LinearSVC(dual = 'auto'))\n",
    "    svm1.fit(svm_train, y_train)\n",
    "\n",
    "    # Represent Test images as histograms\n",
    "    svm_test = []\n",
    "    for i in range(test_dataset.data.shape[0]):\n",
    "        histogram = image_histogram_represent(test_dataset.data[i], kmeans, k, cluster_centers=cluster_centers)\n",
    "        svm_test.append(histogram)\n",
    "\n",
    "    # Find out the Classification Accuracy of the Model\n",
    "    y_pred = svm1.predict(svm_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"-->\\tAccuracy = {accuracy}\")\n",
    "    accuracies.append(accuracy)\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oNMUXvWvy5Z",
    "outputId": "a48a95bf-6274-448c-b109-94b7424738a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(k_list, accuracies)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqlAmoUzvy5Z"
   },
   "source": [
    "We observe that as the Number of Clusters increases, the accuracy of predictions using the BoVW Approach also increases. But, after a fixed value of k (number of cluster centers), increase in accuracy of predictions reduces and it converges to an almost fixed value. But, for lower values of k, the increase is sharp. It points out the Limitations of Classification using SIFT Detector and Descriptor. However, still the SIFT Detector Approach is able to Capture amost 70% of features which is a decent accuracy given the fact that it's execution time is very low. So, if we want to only know the regions of local features, SIFT Detector and Descriptor gives us a fair idea of that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjVExXnYvy5Z"
   },
   "source": [
    "##### 1.3 Impact of different Hyperparameters #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_s1E84jvy5Z",
    "outputId": "fd20e9ea-2689-468d-e91e-caa626d0307a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1\tLoss Function = squared_hinge\t-->\tAccuracy = 0.6535\n",
      "C = 1\tLoss Function = hinge\t-->\tAccuracy = 0.5994\n",
      "C = 0.5\tLoss Function = squared_hinge\t-->\tAccuracy = 0.6542\n",
      "C = 0.5\tLoss Function = hinge\t-->\tAccuracy = 0.5471\n",
      "C = 1.5\tLoss Function = squared_hinge\t-->\tAccuracy = 0.6444\n",
      "C = 1.5\tLoss Function = hinge\t-->\tAccuracy = 0.6431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6535, 0.5994, 0.6542, 0.5471, 0.6444, 0.6431]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "c_list = [1, 0.5, 1.5]\n",
    "loss_list = ['squared_hinge', 'hinge']\n",
    "k = 50\n",
    "accuracies = []\n",
    "for c in c_list:\n",
    "    for loss_function in loss_list:\n",
    "        print(f\"C = {c}\\tLoss Function = {loss_function}\", end = '\\t')\n",
    "\n",
    "        # Fit K-Means on the obtained Descriptors\n",
    "        kmeans, cluster_centers = cluster_sift_descriptors(np.array(descriptors), k)\n",
    "\n",
    "        # Represent Train images as histograms\n",
    "        svm_train = []\n",
    "        for i in range(train_dataset.data.shape[0]):\n",
    "            histogram = image_histogram_represent(train_dataset.data[i], kmeans, k, cluster_centers=cluster_centers)\n",
    "            svm_train.append(histogram)\n",
    "\n",
    "        svm1 = make_pipeline(StandardScaler(), svm.LinearSVC(dual = 'auto', C = c, loss = loss_function))\n",
    "        svm1.fit(svm_train, y_train)\n",
    "\n",
    "        # Represent Test images as histograms\n",
    "        svm_test = []\n",
    "        for i in range(test_dataset.data.shape[0]):\n",
    "            histogram = image_histogram_represent(test_dataset.data[i], kmeans, k, cluster_centers=cluster_centers)\n",
    "            svm_test.append(histogram)\n",
    "\n",
    "        # Find out the Classification Accuracy of the Model\n",
    "        y_pred = svm1.predict(svm_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"-->\\tAccuracy = {accuracy}\")\n",
    "        accuracies.append(accuracy)\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxBHM8olvy5Z"
   },
   "source": [
    "We observe that keeping the Number of Clusters constant from the above part (k = 50), Accuracies which are obtained by varying the Regularisation Parameter and Loss Function are more or less the same. It essentially means that the Number of Clusters (k) in this algorithm is the significant hyperparameter. As value of k increases, the Linear SVM along with K-means Clustering is able to capture the Local Regions of arious features in the Images which leads to better classification accuracies. Although, we observe that in general squared_hinge loss function (which is implemented as default in sklearn) outperforms hinge loss function in almost all the scenarios. The accuracies obtained using squared_hinge loss function are nearly 65% while those obtained by hinge loss function are generally around 55-60% with some exceptions of values of regularisation parameter C. However, in all cases, Accuracy of squared hinge loss function is greater than that of hinge loss function. So, in general, squared hinge loss function is better for classification than the hinge loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CNNs and Vision Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FspaEVOqvy5Z"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyEl2_vdvy5Z",
    "outputId": "00075eb3-c1fa-4200-bdb7-ab89a2117113"
   },
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "torch.manual_seed = 42\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Split the training set into training and validation sets and make separate dataloaders for each\n",
    "train_size = int(0.7 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "trainset, valset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
    "\n",
    "testset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fX9zUir1vy5Z"
   },
   "source": [
    "#### 2.1 Training MNIST with LeNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tQ_U_Svjvy5Z"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5, padding = 2, stride = 1),                   #(N, 1, 28, 28) -> (N, 6, 28, 28)\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, stride=2),           #(N,6,28,28) -> (N,6,14,14)\n",
    "            nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = 5),                  #(N,6,14,14) -> (N,16,10,10)\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(2, stride=2),            #(N,16,10,10) -> (N,16,5,5)\n",
    "            nn.Flatten(1, 3),                    #(N,16,5,5) -> (N,400)\n",
    "            nn.Linear(400, 120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "8c4f00ce9ab14568b94803b0cec0790d",
      "57131082fbd947ad9ecde564b2e29216",
      "27f958088ae34beaa12413804467f12d",
      "b39003983eed4293af15c62feaf57962",
      "1f0d221424314981a04f2ad47bef3938",
      "de6535eb3e3041cb81bdcfe72bbd68a8",
      "8e03accca3b94a02bb5814c7b253609f",
      "23495a7933b743b395cf93f170aeda54"
     ]
    },
    "id": "i0KmOQAXvy5Z",
    "outputId": "a1e7a6f2-8180-432e-c891-d5815a4ec682"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpranav-g\u001b[0m (\u001b[33mpranavg1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4f00ce9ab14568b94803b0cec0790d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011125088400000196, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20240307_163148-3ktlx9ng</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pranavg1/CV_Assignment2-Q2/runs/3ktlx9ng' target=\"_blank\">morning-wave-1</a></strong> to <a href='https://wandb.ai/pranavg1/CV_Assignment2-Q2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pranavg1/CV_Assignment2-Q2' target=\"_blank\">https://wandb.ai/pranavg1/CV_Assignment2-Q2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pranavg1/CV_Assignment2-Q2/runs/3ktlx9ng' target=\"_blank\">https://wandb.ai/pranavg1/CV_Assignment2-Q2/runs/3ktlx9ng</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/pranavg1/CV_Assignment2-Q2/runs/3ktlx9ng?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7a2d49427280>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"CV_Assignment2-Q2\",\n",
    "    config={\n",
    "        \"optimizer\": \"gd\",\n",
    "        \"loss\": \"crossentropy\",\n",
    "        \"metric\": \"accuracy\",\n",
    "        \"epoch\": 1000\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yksNeYzzvy5Z",
    "outputId": "f686a3b2-6299-4ed2-8032-ad37376e8501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\tTraining Loss = 0.005259239114820957\tAccuracy = 90.27619047619048\n",
      "Epoch = 0\tValidation Loss = 0.002170537831261754\tAccuracy = 92.005\n",
      "Epoch = 1\tTraining Loss = 0.0017152332002297044\tAccuracy = 96.62619047619047\n",
      "Epoch = 1\tValidation Loss = 0.001339772716164589\tAccuracy = 96.85166666666667\n",
      "Epoch = 2\tTraining Loss = 0.001109174219891429\tAccuracy = 97.84285714285714\n",
      "Epoch = 2\tValidation Loss = 0.0011063007405027747\tAccuracy = 97.84833333333333\n",
      "Epoch = 3\tTraining Loss = 0.0008043526904657483\tAccuracy = 98.42619047619047\n",
      "Epoch = 3\tValidation Loss = 0.0010261328425258398\tAccuracy = 98.3\n",
      "Epoch = 4\tTraining Loss = 0.0006159242475405335\tAccuracy = 98.83809523809524\n",
      "Epoch = 4\tValidation Loss = 0.0009895190596580505\tAccuracy = 98.61666666666666\n",
      "Epoch = 5\tTraining Loss = 0.00047752956743352115\tAccuracy = 99.08333333333333\n",
      "Epoch = 5\tValidation Loss = 0.0009834886295720935\tAccuracy = 98.79\n",
      "Epoch = 6\tTraining Loss = 0.0003872183442581445\tAccuracy = 99.2452380952381\n",
      "Epoch = 6\tValidation Loss = 0.0009283982217311859\tAccuracy = 98.94\n",
      "Epoch = 7\tTraining Loss = 0.0003326399310026318\tAccuracy = 99.36428571428571\n",
      "Epoch = 7\tValidation Loss = 0.0010502759832888842\tAccuracy = 98.98666666666666\n",
      "Epoch = 8\tTraining Loss = 0.00029870480648241937\tAccuracy = 99.42619047619047\n",
      "Epoch = 8\tValidation Loss = 0.0009743106784299016\tAccuracy = 99.07833333333333\n",
      "Epoch = 9\tTraining Loss = 0.00024333452165592462\tAccuracy = 99.51904761904763\n",
      "Epoch = 9\tValidation Loss = 0.0010243457509204745\tAccuracy = 99.15\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x).to(device)\n",
    "        loss = criterion(y_pred, y)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.shape[0]\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch = {epoch}\\tTraining Loss = {train_loss/len(trainset)}\\tAccuracy = {(correct*100)/total}\")\n",
    "    wandb.log({\"Epoch\": epoch, \"Average Training Loss: \": train_loss/len(trainset), \"Training Accuracy: \": (correct*100)/total})\n",
    "\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valloader):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.shape[0]\n",
    "            val_loss += loss\n",
    "    print(f\"Epoch = {epoch}\\tValidation Loss = {val_loss/len(valset)}\\tAccuracy = {(correct*100)/total}\")\n",
    "    wandb.log({\"Epoch\": epoch, \"Average Validation Loss: \": val_loss/len(valset), \"Validation Accuracy: \": (correct*100)/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WL3qUHg1vy5Z",
    "outputId": "bf825d5a-5101-47e6-efaa-969be537bb6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss = 0.0010411626426503062\tAccuracy = 98.24\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "for i, batch in enumerate(testloader):\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x).to(device)\n",
    "    loss = criterion(y_pred, y)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct += (predicted == y).sum().item()\n",
    "    total += y.shape[0]\n",
    "    test_loss += loss\n",
    "print(f\"Testing Loss = {test_loss/len(testset)}\\tAccuracy = {(correct*100)/total}\")\n",
    "wandb.log({\"Average Testing Loss: \": val_loss/len(valset), \"Testing Accuracy: \": (correct*100)/total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460,
     "referenced_widgets": [
      "7256a0d21d3b4e54978027592eed8a6c",
      "f009d2e5f53c4caca15ac74d0cf3342b",
      "afafca885a5e419d9002b40964b851fa",
      "abf81454198e4a36bc152500a418c40f",
      "62ead50fe4214fabad0af6a9f76ae8f2",
      "b4bc78a73e774b6e868bf599c88007c5",
      "d8d70370f66744efa0c751908bc7f603",
      "ea3b31942e23469882c467d4b6729b7c"
     ]
    },
    "id": "aoJHm9divy5Z",
    "outputId": "cb370abd-a720-48cb-9cd4-70ef9c0be9d7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7256a0d21d3b4e54978027592eed8a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Average Testing Loss: </td><td>▁</td></tr><tr><td>Average Training Loss: </td><td>██▃▂▂▂▁▁▁▁▁</td></tr><tr><td>Average Validation Loss: </td><td>█▃▂▂▁▁▁▂▁▂</td></tr><tr><td>Epoch</td><td>▁▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>Testing Accuracy: </td><td>▁</td></tr><tr><td>Training Accuracy: </td><td>▁▁▆▇▇▇█████</td></tr><tr><td>Validation Accuracy: </td><td>▁▆▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Average Testing Loss: </td><td>0.00102</td></tr><tr><td>Average Training Loss: </td><td>0.00024</td></tr><tr><td>Average Validation Loss: </td><td>0.00102</td></tr><tr><td>Epoch</td><td>9</td></tr><tr><td>Testing Accuracy: </td><td>98.24</td></tr><tr><td>Training Accuracy: </td><td>99.51905</td></tr><tr><td>Validation Accuracy: </td><td>99.15</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">morning-wave-1</strong> at: <a href='https://wandb.ai/pranavg1/CV_Assignment2-Q2/runs/3ktlx9ng' target=\"_blank\">https://wandb.ai/pranavg1/CV_Assignment2-Q2/runs/3ktlx9ng</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240307_163148-3ktlx9ng/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBN-qjXyvy5Z"
   },
   "source": [
    "#### 2.2 Hyperparameter Tuning ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "irfXuvC4vy5Z"
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tune(optimizer, criterion, trainloader, valloader, testloader):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        train_loss = 0\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x).to(device)\n",
    "            loss = criterion(y_pred, y)\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.shape[0]\n",
    "            train_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(valloader):\n",
    "                x, y = batch\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                _, predicted = torch.max(y_pred, 1)\n",
    "                correct += (predicted == y).sum().item()\n",
    "                total += y.shape[0]\n",
    "                val_loss += loss\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    for i, batch in enumerate(testloader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x).to(device)\n",
    "        loss = criterion(y_pred, y)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.shape[0]\n",
    "        test_loss += loss\n",
    "    print(f\"Testing Loss = {test_loss/len(testset)}\\tAccuracy = {(correct*100)/total}\")\n",
    "    return correct*100/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLtiOPOVvy5Z",
    "outputId": "23c83e52-1f73-44a3-a98c-3fc9dd52c11b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss = 0.0013571350136771798\tAccuracy = 97.38\n",
      "Testing Loss = 0.0006915962439961731\tAccuracy = 98.62\n",
      "Testing Loss = 0.0326070636510849\tAccuracy = 47.4\n",
      "Testing Loss = 0.0021132458932697773\tAccuracy = 97.81\n",
      "Testing Loss = 0.0022312100045382977\tAccuracy = 97.98\n",
      "Testing Loss = 0.031440217047929764\tAccuracy = 73.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[97.38, 98.62, 47.4, 97.81, 97.98, 73.7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "accuracies = []\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "accuracies.append(hyperparameter_tune(optimizer=optimizer, criterion=criterion, trainloader=trainloader, valloader=valloader, testloader=testloader))\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "accuracies.append(hyperparameter_tune(optimizer=optimizer, criterion=criterion, trainloader=trainloader, valloader=valloader, testloader=testloader))\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0005)\n",
    "accuracies.append(hyperparameter_tune(optimizer=optimizer, criterion=criterion, trainloader=trainloader, valloader=valloader, testloader=testloader))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "accuracies.append(hyperparameter_tune(optimizer=optimizer, criterion=criterion, trainloader=trainloader, valloader=valloader, testloader=testloader))\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "accuracies.append(hyperparameter_tune(optimizer=optimizer, criterion=criterion, trainloader=trainloader, valloader=valloader, testloader=testloader))\n",
    "\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0005)\n",
    "accuracies.append(hyperparameter_tune(optimizer=optimizer, criterion=criterion, trainloader=trainloader, valloader=valloader, testloader=testloader))\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWUgEvJBvy5a"
   },
   "source": [
    "We observe that as learning rate becomes closer to the optimal learning rate (in this case optimal learning is closer to 0.0001), it leads to faster convergence and therefore better convergence. Also, it is observed that Adam Optimizer outperforms the perfomance of SGD Optimizer since Adam Optimizer converges much faster than SGD Optimizer, and is more stable at the same time. With this we conclude that larger Batch Size leads to lower running time of the loops but leads to almost the same accuracy. At the same time, choice of Optimizer plays an important role for training any model and Adam Optimizer converges and produces great results in very short amount of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Nd8IZDbvy5a"
   },
   "source": [
    "#### 2.3 Comparison ####\n",
    "We notice that Best CNN Model gives much better results compared to Best SIFT-SVM BoVW Model. It is because CNN Model gives accuracy of 98-99% accuracies while, on the other hand, SIFT Model gives us accuracies of only 60-65%. SIFT Detector and Descriptor is not able to capture the localized features in any given region of the image to that extent as the CNN based Model, because convolution operators with different Kernels with the entire Image thus is able to capture the various localized regions in the image because of varied intensities and RGB Values of different pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XD3v7N7Fvy5a"
   },
   "source": [
    "#### 2.4 Performance change as number of Convolutional Layers double ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "op9711h0vy5a"
   },
   "outputs": [],
   "source": [
    "class CNN_new(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_new,self).__init__()\n",
    "        self.cnn_model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5, padding = 2),             # (N, 1, 28, 28) -> (N, 6, 28, 28)\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(in_channels = 6, out_channels = 6, kernel_size = 5, padding = 2),             # (N, 6, 28, 28) -> (N, 6, 28, 28)\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size = 2, stride = 2),                  # (N, 6, 28, 28) -> (N, 6, 14, 14)\n",
    "            nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = 5, padding = 0),            # (N, 6, 14, 14) -> (N, 16, 10, 10)\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = 5, padding = 2),           # (N, 16, 10, 10) -> (N, 16, 10, 10)\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size = 2, stride = 2),                  # (N, 16, 10, 10) -> (N, 16, 5, 5)\n",
    "            nn.Flatten(1, 3),                   # (N, 16, 5, 5) -> (N, 400)\n",
    "            nn.Linear(400, 120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2o6jEBSvy5a",
    "outputId": "08c8eb7c-3941-4de1-b3d3-4f1a62910d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss = 0.0015247208066284657\tAccuracy = 98.57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.57"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = CNN_new().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "hyperparameter_tune(optimizer=optimizer, criterion=criterion, trainloader=trainloader, valloader=valloader, testloader=testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cygXChT4vy5a"
   },
   "source": [
    "We observe that on doubling the No. of Convolutional Layers, Accuracy remains almost the same, it remains almost the same and decreases slightly from 98.62% to 98.57%, which is almost the same. That is, the original LeNet Architecture alone is sufficient to capture the Local Features across all regions in the image and addition of extra convolutional layers only adds on to the No. of operations and does not contribute much to the feature capturing power of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vH2JEjPgvy5a"
   },
   "source": [
    "#### 2.5 Performance change with increase in training samples ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mnist(x):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "\n",
    "    for i in range(10):\n",
    "        class_indices = torch.where(train_dataset.targets == i)[0]\n",
    "        class_samples = train_dataset.data[class_indices]\n",
    "        y = []\n",
    "        X_train.append(class_samples[:int(x/6)])\n",
    "        [y.append(i) for _ in range(int(x/6))]\n",
    "        Y_train.append(torch.tensor(y))\n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "class Custom_Subset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = transforms.ToPILImage()(x)\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYUivIojvy5a",
    "outputId": "52c492d5-032c-4b5d-e306-8cd59277dd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Samples = 600\t-->\tTesting Loss = 0.009114941582083702\tAccuracy = 84.13\n",
      "Number of Training Samples = 1800\t-->\tTesting Loss = 0.004188993945717812\tAccuracy = 91.92\n",
      "Number of Training Samples = 6000\t-->\tTesting Loss = 0.0019518779590725899\tAccuracy = 96.01\n",
      "Number of Training Samples = 18000\t-->\tTesting Loss = 0.0010700274724513292\tAccuracy = 97.98\n",
      "Number of Training Samples = 60000\t-->\tTesting Loss = 0.0009402282303199172\tAccuracy = 98.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[84.13, 91.92, 96.01, 97.98, 98.4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "samples_l = np.array([600, 1800, 6000, 18000, 60000])\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "accuracies = []\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize the image\n",
    "])\n",
    "\n",
    "for n_samples in samples_l:\n",
    "    print(f\"Number of Training Samples = {n_samples}\", end = \"\\t-->\\t\")\n",
    "    X_train = None\n",
    "    Y_train = None\n",
    "    if (n_samples == 60000):\n",
    "        X_train = train_dataset.data\n",
    "        Y_train = train_dataset.targets\n",
    "    else:\n",
    "        x, y = sample_mnist(n_samples)\n",
    "        x_train = torch.stack(x)\n",
    "        y_train = torch.stack(y)\n",
    "        X_train = x_train.view(x_train.shape[0]*x_train.shape[1], 28, 28)\n",
    "        Y_train = y_train.view(y_train.shape[0]*y_train.shape[1])\n",
    "\n",
    "\n",
    "    custom_dataset = Custom_Subset(x=X_train, y=Y_train, transform=transform)\n",
    "    train_size = int(0.7 * len(custom_dataset))\n",
    "    val_size = len(custom_dataset) - train_size\n",
    "    train_set, val_set = torch.utils.data.random_split(custom_dataset, [train_size, val_size])\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=False)\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "    accuracy = hyperparameter_tune(optimizer=optimizer, criterion=criterion, trainloader=trainloader, valloader=valloader, testloader=testloader)\n",
    "    accuracies.append(accuracy)\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbKaxxVdvy5a"
   },
   "source": [
    "We observe that as the Number of Training Samples increases, model is able to generalise better and the Accuracy on the Testing Set increase from 84% to 98%. This means that as the Number of Samples increases Model sees the Local Regions corresponding to different labels in a better fashion and is thus able to capture the Features in a better way, which leads to great generalisation on the Testing Set as well, leading to High Accuracies. However, after the certain point, the increase in number of training set size is not much and what matters is the Architecture of the Model and the Complexity of the Features to be captured in the Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KehNPrWevy5a"
   },
   "source": [
    "#### 2.6 Vision Transformers ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WJhSJhNhvy5h"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class VIT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VIT, self).__init__()\n",
    "        self.linear_proj = nn.Linear(49, 32)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model = 32, nhead = 4, batch_first = True), num_layers = 2)\n",
    "        # self.encoders = nn.ModuleList([VIT_Encoder() for _ in range(2)])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 32)).to(device)\n",
    "\n",
    "        self.mlp = nn.Linear(32, 10)\n",
    "        self.create_pos_embed()\n",
    "        return\n",
    "    \n",
    "\n",
    "    def create_pos_embed(self):\n",
    "        self.pos_embeddings = torch.zeros(64, 17, 32, dtype = torch.float32, device = device)\n",
    "        self.pos_embeddings.requires_grad = True\n",
    "        with torch.no_grad():\n",
    "            for j in range(17):\n",
    "                for k in range(32):\n",
    "                    self.pos_embeddings[:, j, k] = math.sin(j/math.pow(10000, k/32)) if k%2 == 0 else math.cos(j/math.pow(10000, (k-1)/32))\n",
    "        return self.pos_embeddings\n",
    "\n",
    "\n",
    "    def image_patches(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        num_patches = 4\n",
    "        patches = torch.zeros(n, num_patches ** 2, h * w * c // num_patches ** 2).to(device)\n",
    "        patch_size = h // num_patches\n",
    "\n",
    "        for index, image in enumerate(x):\n",
    "            for i in range(num_patches):\n",
    "                for j in range(num_patches):\n",
    "                    patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
    "                    patches[index, i * num_patches + j] = patch.flatten()\n",
    "        return patches\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "\n",
    "        # Divide the image into patches and create Patch Embeddings\n",
    "        self.patch_embeddings = self.image_patches(x)\n",
    "\n",
    "        # Linear Projection of Patch Embeddings\n",
    "        self.patch_embeddings = self.linear_proj(self.patch_embeddings)\n",
    "\n",
    "        # Add a Class Token at the beginning\n",
    "        self.patch_embeddings = torch.cat((cls_tokens, self.patch_embeddings), dim=1)\n",
    "\n",
    "        # Add Position Embeddings to the Patch Embeddings\n",
    "        self.patch_embeddings += self.pos_embeddings[: self.patch_embeddings.shape[0]]\n",
    "\n",
    "        # Pass the Embeddings to the Transformer\n",
    "        self.patch_embeddings = self.transformer(self.patch_embeddings)\n",
    "\n",
    "\n",
    "        # Take the Learned Class Token representing the imformation of the Image\n",
    "        self.patch_embeddings = self.patch_embeddings[:, 0, :]\n",
    "\n",
    "        # Pass the Output Class Token to the MLP Head (Decoder)\n",
    "        return self.mlp(self.patch_embeddings).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UDuqOIGDvy5h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\tTraining Loss = 0.014267203398048878\tAccuracy = 69.00714285714285\n",
      "Epoch = 0\tValidation Loss = 0.005421002395451069\tAccuracy = 75.22333333333333\n",
      "Epoch = 1\tTraining Loss = 0.004641305189579725\tAccuracy = 90.88809523809523\n",
      "Epoch = 1\tValidation Loss = 0.0037906812503933907\tAccuracy = 91.445\n",
      "Epoch = 2\tTraining Loss = 0.003372100880369544\tAccuracy = 93.5047619047619\n",
      "Epoch = 2\tValidation Loss = 0.0031441771425306797\tAccuracy = 93.65\n",
      "Epoch = 3\tTraining Loss = 0.003012078581377864\tAccuracy = 94.02857142857142\n",
      "Epoch = 3\tValidation Loss = 0.002916199853643775\tAccuracy = 94.10333333333334\n",
      "Epoch = 4\tTraining Loss = 0.002621325897052884\tAccuracy = 94.86190476190477\n",
      "Epoch = 4\tValidation Loss = 0.002767516067251563\tAccuracy = 94.73166666666667\n",
      "Epoch = 5\tTraining Loss = 0.0023806574754416943\tAccuracy = 95.35238095238095\n",
      "Epoch = 5\tValidation Loss = 0.0030075209215283394\tAccuracy = 95.0\n",
      "Epoch = 6\tTraining Loss = 0.0022212278563529253\tAccuracy = 95.62857142857143\n",
      "Epoch = 6\tValidation Loss = 0.0025837295688688755\tAccuracy = 95.44666666666667\n",
      "Epoch = 7\tTraining Loss = 0.0020956797525286674\tAccuracy = 95.93333333333334\n",
      "Epoch = 7\tValidation Loss = 0.002640689490363002\tAccuracy = 95.65333333333334\n",
      "Epoch = 8\tTraining Loss = 0.0018896182300522923\tAccuracy = 96.11666666666666\n",
      "Epoch = 8\tValidation Loss = 0.0023426285479217768\tAccuracy = 95.96833333333333\n",
      "Epoch = 9\tTraining Loss = 0.001852537738159299\tAccuracy = 96.32619047619048\n",
      "Epoch = 9\tValidation Loss = 0.002531085629016161\tAccuracy = 95.995\n",
      "Epoch = 10\tTraining Loss = 0.0017546680755913258\tAccuracy = 96.46904761904761\n",
      "Epoch = 10\tValidation Loss = 0.002295562531799078\tAccuracy = 96.22666666666667\n",
      "Epoch = 11\tTraining Loss = 0.0016114790923893452\tAccuracy = 96.75238095238095\n",
      "Epoch = 11\tValidation Loss = 0.0024261900689452887\tAccuracy = 96.365\n",
      "Epoch = 12\tTraining Loss = 0.0016093391459435225\tAccuracy = 96.79285714285714\n",
      "Epoch = 12\tValidation Loss = 0.002386383479461074\tAccuracy = 96.41666666666667\n",
      "Epoch = 13\tTraining Loss = 0.0014723771018907428\tAccuracy = 97.08095238095238\n",
      "Epoch = 13\tValidation Loss = 0.002311032498255372\tAccuracy = 96.61333333333333\n",
      "Epoch = 14\tTraining Loss = 0.0014306214870885015\tAccuracy = 97.07619047619048\n",
      "Epoch = 14\tValidation Loss = 0.0020890627056360245\tAccuracy = 96.74\n",
      "Epoch = 15\tTraining Loss = 0.001352684572339058\tAccuracy = 97.23095238095237\n",
      "Epoch = 15\tValidation Loss = 0.0019373325631022453\tAccuracy = 96.975\n",
      "Epoch = 16\tTraining Loss = 0.0013494831509888172\tAccuracy = 97.27857142857142\n",
      "Epoch = 16\tValidation Loss = 0.0018744943663477898\tAccuracy = 97.02833333333334\n",
      "Epoch = 17\tTraining Loss = 0.0012803809950128198\tAccuracy = 97.41666666666667\n",
      "Epoch = 17\tValidation Loss = 0.002040667226538062\tAccuracy = 97.06166666666667\n",
      "Epoch = 18\tTraining Loss = 0.0012246336555108428\tAccuracy = 97.49047619047619\n",
      "Epoch = 18\tValidation Loss = 0.0018109622178599238\tAccuracy = 97.22333333333333\n",
      "Epoch = 19\tTraining Loss = 0.0012071862583979964\tAccuracy = 97.58095238095238\n",
      "Epoch = 19\tValidation Loss = 0.0019366980995982885\tAccuracy = 97.21333333333334\n",
      "Epoch = 20\tTraining Loss = 0.001128177042119205\tAccuracy = 97.74047619047619\n",
      "Epoch = 20\tValidation Loss = 0.00210812920704484\tAccuracy = 97.26\n",
      "Epoch = 21\tTraining Loss = 0.0010857497109100223\tAccuracy = 97.74285714285715\n",
      "Epoch = 21\tValidation Loss = 0.001975924475118518\tAccuracy = 97.31166666666667\n",
      "Epoch = 22\tTraining Loss = 0.0010656246449798346\tAccuracy = 97.8452380952381\n",
      "Epoch = 22\tValidation Loss = 0.001905131503008306\tAccuracy = 97.475\n",
      "Epoch = 23\tTraining Loss = 0.0010735816322267056\tAccuracy = 97.81666666666666\n",
      "Epoch = 23\tValidation Loss = 0.0018429459305480123\tAccuracy = 97.42\n",
      "Epoch = 24\tTraining Loss = 0.0010454991133883595\tAccuracy = 97.81190476190476\n",
      "Epoch = 24\tValidation Loss = 0.001922793686389923\tAccuracy = 97.47166666666666\n",
      "Epoch = 25\tTraining Loss = 0.0009783066343516111\tAccuracy = 98.01190476190476\n",
      "Epoch = 25\tValidation Loss = 0.002093761693686247\tAccuracy = 97.48\n",
      "Epoch = 26\tTraining Loss = 0.001032144413329661\tAccuracy = 97.80952380952381\n",
      "Epoch = 26\tValidation Loss = 0.0018288006540387869\tAccuracy = 97.41666666666667\n",
      "Epoch = 27\tTraining Loss = 0.0009322456899099052\tAccuracy = 98.08095238095238\n",
      "Epoch = 27\tValidation Loss = 0.0019205076387152076\tAccuracy = 97.60833333333333\n",
      "Epoch = 28\tTraining Loss = 0.0009231175645254552\tAccuracy = 98.06666666666666\n",
      "Epoch = 28\tValidation Loss = 0.001955703366547823\tAccuracy = 97.62\n",
      "Epoch = 29\tTraining Loss = 0.0009060460142791271\tAccuracy = 98.12857142857143\n",
      "Epoch = 29\tValidation Loss = 0.0018356931395828724\tAccuracy = 97.67\n",
      "Testing Loss = 0.0010559391230344772\tAccuracy = 98.14\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = VIT().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model.train()\n",
    "\n",
    "for epoch in range(30):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x).to(device)\n",
    "        loss = criterion(y_pred, y)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.shape[0]\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch = {epoch}\\tTraining Loss = {train_loss/len(trainset)}\\tAccuracy = {(correct*100)/total}\")\n",
    "\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valloader):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x).to(device)\n",
    "            loss = criterion(y_pred, y)\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.shape[0]\n",
    "            val_loss += loss\n",
    "    print(f\"Epoch = {epoch}\\tValidation Loss = {val_loss/len(valset)}\\tAccuracy = {(correct*100)/total}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "for i, batch in enumerate(testloader):\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x).to(device)\n",
    "    loss = criterion(y_pred, y)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct += (predicted == y).sum().item()\n",
    "    total += y.shape[0]\n",
    "    test_loss += loss\n",
    "print(f\"Testing Loss = {test_loss/len(testset)}\\tAccuracy = {(correct*100)/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\tTraining Loss = 0.0055080596357584\tAccuracy = 21.34285714285714\n",
      "Epoch = 0\tValidation Loss = 0.003913457505404949\tAccuracy = 28.85\n",
      "Epoch = 1\tTraining Loss = 0.0029454254545271397\tAccuracy = 61.98571428571429\n",
      "Epoch = 1\tValidation Loss = 0.0021983040496706963\tAccuracy = 65.21\n",
      "Epoch = 2\tTraining Loss = 0.0018357711378484964\tAccuracy = 77.64285714285714\n",
      "Epoch = 2\tValidation Loss = 0.0015178112080320716\tAccuracy = 79.27\n",
      "Epoch = 3\tTraining Loss = 0.001386373653076589\tAccuracy = 83.67142857142858\n",
      "Epoch = 3\tValidation Loss = 0.0012853516964241862\tAccuracy = 84.34\n",
      "Epoch = 4\tTraining Loss = 0.001082634087651968\tAccuracy = 87.32857142857142\n",
      "Epoch = 4\tValidation Loss = 0.0010933883022516966\tAccuracy = 87.51\n",
      "Epoch = 5\tTraining Loss = 0.000948504195548594\tAccuracy = 89.25714285714285\n",
      "Epoch = 5\tValidation Loss = 0.0009630670538172126\tAccuracy = 89.25\n",
      "Epoch = 6\tTraining Loss = 0.0008459769305773079\tAccuracy = 90.15714285714286\n",
      "Epoch = 6\tValidation Loss = 0.0008914180216379464\tAccuracy = 90.22\n",
      "Epoch = 7\tTraining Loss = 0.0007210010080598295\tAccuracy = 91.68571428571428\n",
      "Epoch = 7\tValidation Loss = 0.0008596556726843119\tAccuracy = 91.22\n",
      "Epoch = 8\tTraining Loss = 0.0006548168603330851\tAccuracy = 92.11428571428571\n",
      "Epoch = 8\tValidation Loss = 0.0008842726820148528\tAccuracy = 91.34\n",
      "Epoch = 9\tTraining Loss = 0.0005912334891036153\tAccuracy = 92.88571428571429\n",
      "Epoch = 9\tValidation Loss = 0.0007821503095328808\tAccuracy = 92.37\n",
      "Epoch = 10\tTraining Loss = 0.0005355303292162716\tAccuracy = 93.92857142857143\n",
      "Epoch = 10\tValidation Loss = 0.0007806670619174838\tAccuracy = 93.08\n",
      "Epoch = 11\tTraining Loss = 0.0005258325254544616\tAccuracy = 93.81428571428572\n",
      "Epoch = 11\tValidation Loss = 0.000935990537982434\tAccuracy = 92.44\n",
      "Epoch = 12\tTraining Loss = 0.0005132301594130695\tAccuracy = 93.67142857142858\n",
      "Epoch = 12\tValidation Loss = 0.0007253566291183233\tAccuracy = 93.14\n",
      "Epoch = 13\tTraining Loss = 0.00044010512647219\tAccuracy = 94.85714285714286\n",
      "Epoch = 13\tValidation Loss = 0.0007573168259114027\tAccuracy = 93.8\n",
      "Epoch = 14\tTraining Loss = 0.00041164117283187807\tAccuracy = 95.1\n",
      "Epoch = 14\tValidation Loss = 0.0008329348638653755\tAccuracy = 93.85\n",
      "Epoch = 15\tTraining Loss = 0.0004454457957763225\tAccuracy = 94.85714285714286\n",
      "Epoch = 15\tValidation Loss = 0.0008457409567199647\tAccuracy = 93.48\n",
      "Epoch = 16\tTraining Loss = 0.00042087581823579967\tAccuracy = 95.07142857142857\n",
      "Epoch = 16\tValidation Loss = 0.0006285904091782868\tAccuracy = 94.56\n",
      "Epoch = 17\tTraining Loss = 0.0003810161433648318\tAccuracy = 95.27142857142857\n",
      "Epoch = 17\tValidation Loss = 0.0006780099938623607\tAccuracy = 94.31\n",
      "Epoch = 18\tTraining Loss = 0.0003405669122003019\tAccuracy = 95.91428571428571\n",
      "Epoch = 18\tValidation Loss = 0.000703569792676717\tAccuracy = 94.77\n",
      "Epoch = 19\tTraining Loss = 0.00032163431751541793\tAccuracy = 96.1\n",
      "Epoch = 19\tValidation Loss = 0.0006810755003243685\tAccuracy = 95.07\n",
      "Epoch = 20\tTraining Loss = 0.0003196053730789572\tAccuracy = 96.07142857142857\n",
      "Epoch = 20\tValidation Loss = 0.0006877357955090702\tAccuracy = 95.1\n",
      "Epoch = 21\tTraining Loss = 0.00031804866739548743\tAccuracy = 95.98571428571428\n",
      "Epoch = 21\tValidation Loss = 0.0006835546810179949\tAccuracy = 95.02\n",
      "Epoch = 22\tTraining Loss = 0.0002978348347824067\tAccuracy = 96.28571428571429\n",
      "Epoch = 22\tValidation Loss = 0.0006831333739683032\tAccuracy = 95.19\n",
      "Epoch = 23\tTraining Loss = 0.0003128070675302297\tAccuracy = 95.91428571428571\n",
      "Epoch = 23\tValidation Loss = 0.0006450103246606886\tAccuracy = 95.12\n",
      "Epoch = 24\tTraining Loss = 0.00023965528816916049\tAccuracy = 97.0\n",
      "Epoch = 24\tValidation Loss = 0.0007119156653061509\tAccuracy = 95.63\n",
      "Epoch = 25\tTraining Loss = 0.0002881547552533448\tAccuracy = 96.64285714285714\n",
      "Epoch = 25\tValidation Loss = 0.000663516519125551\tAccuracy = 95.54\n",
      "Epoch = 26\tTraining Loss = 0.0002466395089868456\tAccuracy = 96.68571428571428\n",
      "Epoch = 26\tValidation Loss = 0.0007659097900614142\tAccuracy = 95.35\n",
      "Epoch = 27\tTraining Loss = 0.0002447999140713364\tAccuracy = 96.8\n",
      "Epoch = 27\tValidation Loss = 0.0006903225439600646\tAccuracy = 95.6\n",
      "Epoch = 28\tTraining Loss = 0.0002537560649216175\tAccuracy = 96.7\n",
      "Epoch = 28\tValidation Loss = 0.0005904235295020044\tAccuracy = 95.98\n",
      "Epoch = 29\tTraining Loss = 0.00023052864708006382\tAccuracy = 97.3\n",
      "Epoch = 29\tValidation Loss = 0.0007262117578648031\tAccuracy = 96.03\n",
      "Testing Loss = 0.0032905512489378452\tAccuracy = 94.56\n"
     ]
    }
   ],
   "source": [
    "x, y = sample_mnist(6000)\n",
    "x_train = torch.stack(x)\n",
    "y_train = torch.stack(y)\n",
    "X_train = x_train.view(x_train.shape[0]*x_train.shape[1], 28, 28)\n",
    "Y_train = y_train.view(y_train.shape[0]*y_train.shape[1])\n",
    "\n",
    "custom_dataset = Custom_Subset(x=X_train, y=Y_train, transform=transform)\n",
    "train_size = int(0.7 * len(custom_dataset))\n",
    "val_size = len(custom_dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(custom_dataset, [train_size, val_size])\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=False)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = VIT().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model.train()\n",
    "\n",
    "for epoch in range(30):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x).to(device)\n",
    "        loss = criterion(y_pred, y)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.shape[0]\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch = {epoch}\\tTraining Loss = {train_loss/len(trainset)}\\tAccuracy = {(correct*100)/total}\")\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(valloader):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x).to(device)\n",
    "            loss = criterion(y_pred, y)\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.shape[0]\n",
    "            val_loss += loss\n",
    "    print(f\"Epoch = {epoch}\\tValidation Loss = {val_loss/len(valset)}\\tAccuracy = {(correct*100)/total}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0\n",
    "for i, batch in enumerate(testloader):\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x).to(device)\n",
    "    loss = criterion(y_pred, y)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct += (predicted == y).sum().item()\n",
    "    total += y.shape[0]\n",
    "    test_loss += loss\n",
    "print(f\"Testing Loss = {test_loss/len(testset)}\\tAccuracy = {(correct*100)/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations - We observe that as the Size of Training Set increases, Transformer is able to capture the dependencies of the local regions of the Images amongst themselves. So, if the size of training set increases, we can say that Model after observing the Entire Training Set learns how the various regions correspond to different labels. So, classification accuracies with 60K Sized Image Dataset is higher than those obtained with 6K Sized Dataset. The corresponding accuracies obtained are as follows - 98-99% for 60000 images, 94-95% for 6000 images. We can say that it is because Transformers are Data Hungry and require large datasets for the well functioning of Self-Attention Mechanism.\n",
    "Also, we observe that corresponding accuracies are higher in case of CNNs because CNNs are computationally efficient and capture the Local Features in a very fast manner compared to Transformers. It is because Self-Attention requires a very huge amount of time compared to Convolutional Operators. So, for epochs in the range of 10-20 CNNs capture dependencies faster than Transformers because of the Higher Training time involved in case of Transformers and this is the reason that after these number of epochs, accuracies obtained in case of CNNs are higher. But, after a large number of epochs Transformers capture the Attention Scores of each of the Feature and result in better performance than CNNs in general. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1f0d221424314981a04f2ad47bef3938": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23495a7933b743b395cf93f170aeda54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27f958088ae34beaa12413804467f12d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e03accca3b94a02bb5814c7b253609f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_23495a7933b743b395cf93f170aeda54",
      "value": 1
     }
    },
    "57131082fbd947ad9ecde564b2e29216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f0d221424314981a04f2ad47bef3938",
      "placeholder": "​",
      "style": "IPY_MODEL_de6535eb3e3041cb81bdcfe72bbd68a8",
      "value": "Waiting for wandb.init()...\r"
     }
    },
    "62ead50fe4214fabad0af6a9f76ae8f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7256a0d21d3b4e54978027592eed8a6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f009d2e5f53c4caca15ac74d0cf3342b",
       "IPY_MODEL_afafca885a5e419d9002b40964b851fa"
      ],
      "layout": "IPY_MODEL_abf81454198e4a36bc152500a418c40f"
     }
    },
    "8c4f00ce9ab14568b94803b0cec0790d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57131082fbd947ad9ecde564b2e29216",
       "IPY_MODEL_27f958088ae34beaa12413804467f12d"
      ],
      "layout": "IPY_MODEL_b39003983eed4293af15c62feaf57962"
     }
    },
    "8e03accca3b94a02bb5814c7b253609f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abf81454198e4a36bc152500a418c40f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afafca885a5e419d9002b40964b851fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8d70370f66744efa0c751908bc7f603",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea3b31942e23469882c467d4b6729b7c",
      "value": 1
     }
    },
    "b39003983eed4293af15c62feaf57962": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4bc78a73e774b6e868bf599c88007c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8d70370f66744efa0c751908bc7f603": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de6535eb3e3041cb81bdcfe72bbd68a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea3b31942e23469882c467d4b6729b7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f009d2e5f53c4caca15ac74d0cf3342b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62ead50fe4214fabad0af6a9f76ae8f2",
      "placeholder": "​",
      "style": "IPY_MODEL_b4bc78a73e774b6e868bf599c88007c5",
      "value": "0.012 MB of 0.012 MB uploaded\r"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
